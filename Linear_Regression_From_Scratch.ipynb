{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "#importing necessary libraries\n",
    "#we'll use the make_regression function to generate a fake dataset for the purpose of learning regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, true_coefficients=make_regression(n_samples=1000, n_features=5, n_informative=5, coef=True)\n",
    "#1000 samples are generated, with 5 features and all 5 will be relevant. True coefficients will also be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test=train_test_split(x,y, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now time to start building the Linear Regression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(): #abbreviate so as not to confuse with sklearn's LinearRegression\n",
    "    \n",
    "    def __init__(self, show_learning=False):\n",
    "        self.show_learning=show_learning\n",
    "        return\n",
    "    \n",
    "    def fit(self, predictors, target, learning_rate):\n",
    "        \n",
    "        '''predictors: x values\n",
    "           target: y values\n",
    "           learning_rate: size of step to take in gradient descent'''\n",
    "        \n",
    "        '''First we need to initialize the gradient and feature weights. For simplicity, just set them all to 1'''\n",
    "        \n",
    "        gradient=np.ones(len(predictors.T))\n",
    "        weights=np.ones(len(predictors.T))\n",
    "        \n",
    "        count=0 #keep this count handy to see how quickly the machine learnsl\n",
    "        \n",
    "        '''We will be using the MSE (mean squared error) as the cost function. Calculus (and Andrew Ng) tells\n",
    "        us that this function is convex meaning there is an absolute, global minimum whose slope is 0. We want\n",
    "        to get as close to 0 as possible, so we can set a threshold and loop til we acvhieve that threshold'''\n",
    "        \n",
    "        while np.mean(abs(gradient)) >= 0.001:\n",
    "            \n",
    "            predictions=np.dot(predictors, weights)\n",
    "            #this is equivalent to B1x1+B2x2+...Bnxn for each data point\n",
    "            \n",
    "            MSE=sum((predictions-target)**2) / 2*len(predictors)\n",
    "            \n",
    "            '''Basic steps for Gradient Descent\n",
    "            \n",
    "                1) Take the partial derivative with respect to each feature of the MSE equation\n",
    "                2) Calculate the gradient for each sample and divide by total number of samples\n",
    "                3) Update each feature as follows: weight=weight-learning_rate*avg_gradient\n",
    "                4) Repeat until gradient is within acceptable (0.001) threshold'''\n",
    "            \n",
    "            '''partial_derivative(x1) = (prediction(x1)-y1)*(x1)\n",
    "               avg_gradient(x1)=1/m*sum(predictions(x1)-y1)*x1)'''\n",
    "            \n",
    "            errors=predictions-target #array of all errors\n",
    "            gradient=np.dot(predictors.T, errors)/len(predictors) #find the avg gradient for each featre\n",
    "            weights -= gradient * learning_rate #simultaneously update the weights \n",
    "            count +=1\n",
    "            \n",
    "            if self.show_learning == True and count%20 == 0:\n",
    "                \n",
    "                print(f'MSE: {round(MSE,0)} | iteration: {count} | avg_gradient {round(np.mean(gradient),3)}')\n",
    "                # every 20 iterations report on the learning \n",
    "                \n",
    "        self.weights=weights\n",
    "        self.count=count \n",
    "            \n",
    "        return f\"Fit complete: {count} iterations\"\n",
    "        \n",
    "    def predict(self, test):\n",
    "\n",
    "        return np.dot(test, self.weights)\n",
    "        \n",
    "    def score(self, predictors, target):\n",
    "\n",
    "        SST=sum((target-np.mean(target)**2)) #Sum of Squared Total\n",
    "        predictions=np.dot(predictors, self.weights)\n",
    "        SSE=sum((target-predictions)**2) #Sum of Squared Error\n",
    "        self.R2=1-(SSE/SST)\n",
    "\n",
    "        return self.R2\n",
    "        \n",
    "    def coef(self):\n",
    "        return self.weights\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LR(show_learning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 98887793.0 | iteration: 20 | avg_gradient -7.299\n",
      "MSE: 1227622.0 | iteration: 40 | avg_gradient -0.803\n",
      "MSE: 15962.0 | iteration: 60 | avg_gradient -0.088\n",
      "MSE: 216.0 | iteration: 80 | avg_gradient -0.01\n",
      "MSE: 3.0 | iteration: 100 | avg_gradient -0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fit complete: 101 iterations'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train,y_train, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000674709966"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.45903378, 80.13829818,  6.50589233, 80.6175024 , 99.29270897])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.45896692, 80.13849981,  6.50611185, 80.61931119, 99.29463   ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.68603713e-05, -2.01626471e-04, -2.19523380e-04, -1.80878295e-03,\n",
       "       -1.92103062e-03])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef()-true_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to achieve a near perfect fit using the custom linear regression class. Now let's see how this compares to the sklearn LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_lr=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.45896692, 80.13849981,  6.50611185, 80.61931119, 99.29463   ])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.27897692e-13,  4.26325641e-14,  3.55271368e-15, -1.84741111e-13,\n",
       "       -4.26325641e-14])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lr.coef_-true_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn does a little bit better, but overall our custom function does a really great job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
