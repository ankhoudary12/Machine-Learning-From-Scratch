{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this notebook will be to build the Naive Bayes algorithm from scratch. The algorithm will then be used on the iris dataset and compared to the sklearn implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykhoudary/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['mean', 'var']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=iris.data\n",
    "y=iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test=train_test_split(x,y, random_state=5) #split into test and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the Gaussian Naive Bayes class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        \n",
    "        def separate_classes(X,y):\n",
    "            '''\n",
    "            This function separates features by class\n",
    "            '''\n",
    "            \n",
    "            classes=[[x for x,t in zip(X,y) if t==c] for c in np.unique(y)]\n",
    "            return classes\n",
    "        \n",
    "        \n",
    "        def make_class_dicts(classes, labels):\n",
    "            '''\n",
    "            This function creates dictionaries for each class containing required stats\n",
    "            '''\n",
    "            \n",
    "            num_samples=len(labels)\n",
    "            class_dict=[]\n",
    "\n",
    "            for x,y in zip(classes, np.unique(labels)):\n",
    "                b={}\n",
    "                b['class']=y\n",
    "                b['p_class']=len(x)/num_samples\n",
    "                b['mean']=np.mean(x, axis=0)\n",
    "                b['var']=np.mean(x, axis=0)\n",
    "                class_dict.append(b)\n",
    "\n",
    "            return class_dict\n",
    "        \n",
    "        # for the fit method, the overall goal is to make these class dicts for each of our classes\n",
    "        # we can then use these class dicts to get probabilities that new points belong to each class\n",
    "        \n",
    "        self.classes=separate_classes(X, y)\n",
    "        self.class_dict=make_class_dicts(self.classes, y)\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        def find_proba_for_each_class(point, class_dict):\n",
    "            '''\n",
    "            This function compares a point to each class and returns the prob of belonging to each class\n",
    "            '''\n",
    "            \n",
    "            probs=[]\n",
    "            for y in class_dict:\n",
    "                p = 1/(np.sqrt(2*np.pi*y['var'])) * np.exp((-(point-y['mean'])**2)/(2*y['var']))\n",
    "                p=np.prod(p)*y['p_class']\n",
    "                probs.append((p, y['class']))\n",
    "            return probs\n",
    "        \n",
    "        def get_labels(probs):\n",
    "            '''\n",
    "            This function returns an array with the class label belonging to highest probability \n",
    "            '''\n",
    "            return [max(x, key=lambda x:x[0])[1] for x in probs]\n",
    "        \n",
    "        # the basis of the predict method will be to find the probability that each feature belongs to each class\n",
    "        # then we simply return the class label associated with the greatest probability of occurence \n",
    "        \n",
    "        probs=np.array([find_proba_for_each_class(feature, class_dict) for feature in X ])\n",
    "        \n",
    "        self.predicted_labels=get_labels(probs)\n",
    "        \n",
    "        return self.predicted_labels\n",
    "    \n",
    "    \n",
    "    def score(self, X, y): \n",
    "        \n",
    "        predicted_labels=self.predict(X)\n",
    "        \n",
    "        score=0\n",
    "        \n",
    "        for i in range(len(y)):\n",
    "            \n",
    "            if predicted_labels[i]==y[i]: #check to see if labels match\n",
    "                score+=1\n",
    "                \n",
    "        return score/len(y) #this returns the accuracy (or ratio of correct guesses)\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was able to achieve an accuracuy of 89.5%. Now lets see how this compares to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_nb=naive_bayes.GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_nb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, the sklearn implementation was able to achieve a 92.1% accuracy. This could be due to the smoothing parameter that sklearn implements by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
