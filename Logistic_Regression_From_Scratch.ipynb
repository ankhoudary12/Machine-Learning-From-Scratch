{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#importing necessary libraries\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y=make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, n_classes=2)\n",
    "#make a faux classification problem with 1000 rows, 5 informative features, to classify into 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test=train_test_split(x,y, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to start building the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_regr():\n",
    "    \n",
    "    def __init__(self, lr=0.1, n_iterations=100000, fit_intercept=True, verbose=False):\n",
    "        \n",
    "        self.lr=lr #learning rate (step size to take in gradient descent)\n",
    "        self.n_iterations=n_iterations #max learning steps in gradient descent\n",
    "        self.fit_intercept=True #whether or not to include a column of 1's to use as y-intercept in our hypothesis func\n",
    "        self.verbose=verbose\n",
    "        \n",
    "    def __make_intercept(self, x):\n",
    "        \n",
    "        intercept=np.ones((x.shape[0],1)) #make column of ones equal to length of x\n",
    "        return np.concatenate((intercept, x), axis=1) #join the intercept column \n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        helper function to return the sigmoid function\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        '''\n",
    "        find optimal weights for the given predictors and targets \n",
    "        \n",
    "        '''\n",
    "        #make intercept if necessary\n",
    "        if self.fit_intercept:\n",
    "            x=self.__make_intercept(x) \n",
    "        \n",
    "        \n",
    "        #initialize the gradient and weights with 1's\n",
    "        gradient=np.ones(len(x.T))\n",
    "        weights=np.ones(len(x.T))\n",
    "        \n",
    "        count=0\n",
    "        \n",
    "        '''\n",
    "        the loss function for logistic regression is calculated from Maximum Likelihood estimation and is as follows:\n",
    "        \n",
    "        h=hypothesis function\n",
    "        h=theta_0 + theta_1(x1) + theta_2(x2) + ...\n",
    "        \n",
    "        -1/m*(sum(y*log(h(x))+((1-y)*log(1-h(x)))))\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            \n",
    "            \n",
    "            z=np.dot(x, weights)\n",
    "            \n",
    "            predictions=self.__sigmoid(z) #squeezes the predictions to be between 0 and 1\n",
    "            \n",
    "            loss=sum((y-np.log(predictions)) + ((1-y)*np.log(1-predictions)))\n",
    "            \n",
    "            '''Basic steps for Gradient Descent\n",
    "            \n",
    "                1) Take the partial derivative with respect to each feature of the loss equation\n",
    "                2) Calculate the gradient for each sample and divide by total number of samples\n",
    "                3) Update each feature as follows: weight=weight-learning_rate*avg_gradient\n",
    "                4) Repeat until gradient is within acceptable (0.001) threshold'''\n",
    "            \n",
    "            '''partial_derivative(x1) = (prediction(x1)-y1)*(x1)\n",
    "               avg_gradient(x1)=1/m*sum(predictions(x1)-y1)*x1)'''\n",
    "            \n",
    "            errors=predictions-y\n",
    "            gradient=np.dot(x.T, errors)/len(x)\n",
    "            \n",
    "            weights-=gradient*self.lr\n",
    "            count+=1\n",
    "            \n",
    "            if self.verbose==True and count%20==0:\n",
    "                print(f'MSE: {round(Loss,0)} | iteration: {count} | avg_gradient {round(np.mean(gradient),3)}')\n",
    "                # every 20 iterations report on the learning \n",
    "\n",
    "        self.weights=weights\n",
    "        self.count=count\n",
    "\n",
    "        return f'Fit complete: {count} iterations'\n",
    "        \n",
    "    def predict_proba(self, x):\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            x=self.__make_intercept(x)\n",
    "            \n",
    "        return self.__sigmoid(np.dot(x, self.weights))\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        \n",
    "        return self.predict_proba(x) >= threshold #return 0 if below threshold, 1 if above\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        \n",
    "        predictions=self.predict(x, threshold=0.5)\n",
    "        return (predictions==y).mean() #what ratio of classification labels were correct\n",
    "    \n",
    "    def coef(self):\n",
    "        return self.weights\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=log_regr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fit complete: 100000 iterations'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06128942, 0.81072998, 0.86686304, 0.84581439, 0.55354991,\n",
       "       0.4466992 , 0.01524554, 0.02224619, 0.42960909, 0.30841236,\n",
       "       0.71351839, 0.88889877, 0.8166099 , 0.71515382, 0.21636823,\n",
       "       0.61010688, 0.9761873 , 0.15449802, 0.39787268, 0.41401609,\n",
       "       0.97356467, 0.97051623, 0.32664249, 0.13393816, 0.60496815,\n",
       "       0.26594431, 0.99048351, 0.9009892 , 0.11116134, 0.21201318,\n",
       "       0.1104605 , 0.8025685 , 0.11537804, 0.55640202, 0.29968555,\n",
       "       0.96543288, 0.0087408 , 0.60381635, 0.9033691 , 0.74134486,\n",
       "       0.05915062, 0.05939162, 0.94223685, 0.84211325, 0.82276411,\n",
       "       0.07642754, 0.70345924, 0.35126759, 0.89940217, 0.10311292,\n",
       "       0.02447092, 0.01609298, 0.75067306, 0.81027762, 0.09994359,\n",
       "       0.0114412 , 0.56957   , 0.12443021, 0.64487422, 0.7769182 ,\n",
       "       0.05659   , 0.02659902, 0.3993717 , 0.86749681, 0.01023101,\n",
       "       0.12489605, 0.2088482 , 0.85107546, 0.17503032, 0.54283339,\n",
       "       0.03099943, 0.04927709, 0.16051284, 0.71476147, 0.82634449,\n",
       "       0.35480331, 0.7086751 , 0.12033363, 0.70455967, 0.91787774,\n",
       "       0.1837024 , 0.19923015, 0.7608352 , 0.58434011, 0.91432175,\n",
       "       0.12715547, 0.26510415, 0.34205567, 0.23342631, 0.75473093,\n",
       "       0.84427718, 0.17682945, 0.96035188, 0.91754128, 0.52428557,\n",
       "       0.8655707 , 0.26226117, 0.31024607, 0.02074134, 0.90057783,\n",
       "       0.70770182, 0.89016195, 0.04787848, 0.52318456, 0.4993859 ,\n",
       "       0.76674352, 0.88451605, 0.82363901, 0.26840585, 0.94020895,\n",
       "       0.43854442, 0.56943088, 0.32550588, 0.56020383, 0.36871649,\n",
       "       0.12790238, 0.0453425 , 0.61669018, 0.57819521, 0.35986002,\n",
       "       0.59436598, 0.02094123, 0.94486062, 0.87244741, 0.3370415 ,\n",
       "       0.99892454, 0.44391189, 0.9893059 , 0.83848227, 0.91585132,\n",
       "       0.68115078, 0.73571923, 0.99697637, 0.03625712, 0.38076551,\n",
       "       0.9324768 , 0.4490572 , 0.96003825, 0.03621407, 0.54382743,\n",
       "       0.6922009 , 0.20157128, 0.52010459, 0.19819246, 0.77225798,\n",
       "       0.99944164, 0.8304603 , 0.7492722 , 0.89710319, 0.75732362,\n",
       "       0.85637143, 0.71734021, 0.05415278, 0.74825259, 0.11744307,\n",
       "       0.72572641, 0.07458214, 0.87393937, 0.7392913 , 0.02694275,\n",
       "       0.56667886, 0.53213427, 0.01520083, 0.99066198, 0.96103221,\n",
       "       0.90558844, 0.18795794, 0.26848044, 0.91030155, 0.84000826,\n",
       "       0.89455276, 0.31388053, 0.0230494 , 0.41204587, 0.03435906,\n",
       "       0.30594187, 0.86538561, 0.1187856 , 0.08279835, 0.39702924,\n",
       "       0.22251324, 0.76719108, 0.93482986, 0.25299446, 0.21903152,\n",
       "       0.94046336, 0.33546526, 0.90250795, 0.12547421, 0.9489134 ,\n",
       "       0.58486955, 0.81363984, 0.64057304, 0.99029371, 0.94144371,\n",
       "       0.43950681, 0.48042749, 0.58421019, 0.76503056, 0.72037801,\n",
       "       0.85883747, 0.35306513, 0.78953097, 0.23549042, 0.15520866,\n",
       "       0.01172839, 0.47008497, 0.4657505 , 0.39640979, 0.8568198 ,\n",
       "       0.52037103, 0.42907367, 0.11106707, 0.42419331, 0.44314836,\n",
       "       0.06676992, 0.9475846 , 0.8748798 , 0.79105048, 0.59303619,\n",
       "       0.66942475, 0.56691391, 0.61453643, 0.55020669, 0.32766691,\n",
       "       0.91902739, 0.30217084, 0.70853264, 0.42313891, 0.85047058,\n",
       "       0.89538428, 0.91703247, 0.93117356, 0.33135779, 0.24899563,\n",
       "       0.5947371 , 0.90295208, 0.89780443, 0.73738038, 0.02963797,\n",
       "       0.70215948, 0.03700737, 0.55842331, 0.23900987, 0.64015028,\n",
       "       0.4920851 , 0.99617841, 0.63462249, 0.60822215, 0.27457994])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False, False, False, False, False,\n",
       "       False,  True,  True, False,  True, False,  True,  True, False,\n",
       "        True, False,  True,  True, False, False, False, False,  True,\n",
       "        True,  True, False, False, False, False, False,  True, False,\n",
       "       False,  True, False,  True,  True, False, False, False,  True,\n",
       "        True, False,  True, False,  True,  True,  True,  True, False,\n",
       "        True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True, False, False,  True, False,\n",
       "       False, False,  True,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True, False, False,  True,  True, False,  True,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True, False,  True,\n",
       "        True,  True,  True, False, False,  True, False,  True, False,\n",
       "        True,  True,  True,  True, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False,  True,  True,  True,\n",
       "       False,  True, False, False,  True,  True, False, False,  True,\n",
       "       False, False, False,  True, False,  True,  True, False,  True,\n",
       "        True, False,  True, False,  True, False,  True, False,  True,\n",
       "       False, False, False,  True, False,  True,  True,  True,  True,\n",
       "       False,  True, False,  True,  True,  True, False,  True, False,\n",
       "       False, False,  True, False, False,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True, False, False, False,\n",
       "        True,  True,  True, False, False,  True, False,  True, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "       False, False, False, False,  True, False,  True,  True, False,\n",
       "        True,  True,  True, False,  True, False,  True,  True, False,\n",
       "       False, False,  True, False, False, False,  True,  True, False,\n",
       "        True, False,  True, False, False, False, False, False, False,\n",
       "        True, False, False,  True,  True, False,  True,  True, False,\n",
       "        True,  True, False,  True, False, False, False,  True,  True,\n",
       "        True, False,  True,  True, False,  True,  True, False, False,\n",
       "        True, False,  True, False, False,  True,  True,  True,  True,\n",
       "        True, False,  True, False, False,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True, False,  True, False,\n",
       "       False,  True, False, False,  True, False, False,  True,  True,\n",
       "       False,  True, False, False, False, False,  True, False, False,\n",
       "        True,  True, False, False, False, False,  True,  True, False,\n",
       "       False,  True, False,  True, False,  True,  True, False, False,\n",
       "        True, False, False,  True,  True,  True, False,  True,  True,\n",
       "       False, False,  True, False,  True,  True,  True, False, False,\n",
       "       False, False, False, False, False, False,  True,  True, False,\n",
       "       False, False,  True,  True, False,  True,  True,  True,  True,\n",
       "       False, False,  True, False, False, False, False, False,  True,\n",
       "       False,  True,  True, False, False, False,  True, False, False,\n",
       "        True, False,  True,  True, False, False, False,  True, False,\n",
       "        True,  True,  True, False,  True,  True,  True, False, False,\n",
       "        True,  True, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False,  True,  True,  True,  True,  True,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "        True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True, False, False, False, False,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True,  True, False, False, False,\n",
       "        True, False, False,  True, False,  True, False,  True, False,\n",
       "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
       "        True,  True,  True,  True, False,  True, False, False,  True,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "        True, False,  True, False, False, False,  True, False, False,\n",
       "        True,  True,  True,  True, False, False, False,  True,  True,\n",
       "        True,  True,  True,  True, False,  True, False,  True,  True,\n",
       "       False, False,  True,  True, False, False,  True,  True, False,\n",
       "        True,  True, False, False,  True,  True, False, False,  True,\n",
       "        True, False,  True, False, False, False, False, False, False,\n",
       "        True, False, False,  True,  True, False,  True, False, False,\n",
       "        True, False, False, False, False,  True,  True,  True, False,\n",
       "       False, False,  True, False,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False,  True, False, False,  True,  True,\n",
       "       False,  True, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True, False, False,  True, False,  True,  True,  True, False,\n",
       "        True, False,  True, False,  True, False, False,  True, False,\n",
       "       False, False,  True, False,  True,  True,  True,  True, False,\n",
       "        True,  True,  True, False,  True, False, False,  True,  True,\n",
       "       False,  True,  True, False,  True,  True, False,  True,  True,\n",
       "        True,  True, False, False, False, False, False,  True, False,\n",
       "        True, False, False, False, False,  True,  True, False, False,\n",
       "       False, False, False,  True, False,  True,  True, False, False,\n",
       "       False, False,  True, False,  True,  True, False, False, False,\n",
       "       False, False,  True, False, False,  True,  True, False,  True,\n",
       "       False, False,  True,  True, False, False, False, False,  True,\n",
       "       False,  True,  True,  True, False,  True, False,  True, False,\n",
       "       False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "       False,  True, False,  True,  True, False, False,  True,  True,\n",
       "       False,  True, False,  True, False, False,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True, False,  True, False,\n",
       "        True, False, False,  True,  True, False,  True, False,  True,\n",
       "       False, False, False, False, False, False, False,  True,  True,\n",
       "        True, False,  True,  True, False, False, False,  True,  True,\n",
       "        True, False,  True, False, False, False,  True, False, False,\n",
       "        True,  True, False,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False, False,  True,  True,  True, False,  True,\n",
       "       False,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False, False, False,  True,  True,  True,\n",
       "        True, False, False,  True,  True, False, False,  True, False,\n",
       "       False, False,  True, False, False, False, False, False,  True,\n",
       "        True, False,  True,  True,  True, False, False,  True, False,\n",
       "       False,  True,  True, False,  True,  True, False, False, False,\n",
       "       False,  True, False, False,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False,  True, False,\n",
       "        True,  True, False, False, False, False, False,  True, False,\n",
       "       False, False,  True, False,  True,  True, False, False, False,\n",
       "        True, False, False, False, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True, False, False,  True, False, False, False,\n",
       "       False, False,  True,  True, False, False, False,  True, False,\n",
       "       False, False, False,  True,  True,  True, False, False,  True,\n",
       "        True, False,  True,  True, False,  True,  True,  True, False,\n",
       "        True])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_log=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_log.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_log.score(x_test,y_test) #we performed similarly to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06550428,  0.02378677, -0.1592897 , -0.25692392, -1.21171222]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_log.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00095993, -0.00016801, -0.00080718,  0.00363154,  0.01196109]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_log.coef_-log.coef()[1:] #sklearn doesn't add a fit_intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we were able to implement the Logistic Regression classification model from scratch. We achieved similar performance to sklearn's implementation. Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
